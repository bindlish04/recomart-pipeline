# recomart-pipeline
RecoMart â€“ End-to-End ML Data Pipeline

This repository implements an end-to-end data management and machine learning pipeline for a recommendation system use case (â€œRecoMartâ€).
It demonstrates best practices in data ingestion, validation, preparation, feature engineering, feature stores, data versioning, model training, experiment tracking, and orchestration.

The project was built as part of the Data Management for Machine Learning assignment and is designed to be reproducible, modular, and production-style.

**ğŸ“ŒHigh-level Architecture**
Ingestion (CSV + API)
        â†“
Raw Data Lake (partitioned)
        â†“
Validation & Profiling
        â†“
Preparation & EDA
        â†“
Feature Engineering + Warehouse (SQLite)
        â†“
Feature Store
        â†“
Model Training & Evaluation (MLflow)
        â†“
Orchestration (Prefect)

**ğŸ“ Repository Structure**
recomart-pipeline/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingestion/        # CSV & API ingestion
â”‚   â”œâ”€â”€ validation/       # Data quality checks + reports
â”‚   â”œâ”€â”€ preparation/      # Cleaning & EDA
â”‚   â”œâ”€â”€ transformation/   # Feature engineering + warehouse
â”‚   â”œâ”€â”€ feature_store/    # Feature registry & retrieval
â”‚   â”œâ”€â”€ modeling/         # Model training & evaluation
â”‚   â”œâ”€â”€ orchestration/    # Prefect pipeline
â”‚   â”œâ”€â”€ common/           # Shared utilities (logging)
â”‚   â””â”€â”€ config.py
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ storage_structure.md
â”‚   â”œâ”€â”€ feature_logic.md
â”‚   â”œâ”€â”€ feature_store.md
â”‚   â”œâ”€â”€ dvc_workflow.md
â”‚   â”œâ”€â”€ lineage.md
â”‚   â””â”€â”€ orchestration.md
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw.dvc
â”‚   â”œâ”€â”€ validated.dvc
â”‚   â”œâ”€â”€ prepared.dvc
â”‚   â”œâ”€â”€ features.dvc
â”‚   â””â”€â”€ warehouse.dvc
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .dvc/
â””â”€â”€ .dvcignore

**âš™ï¸ Prerequisites**
Python 3.9+
Git
(Optional) DVC remote access if you want to pull pre-generated data

**ğŸ§ª Setup Instructions
1ï¸âƒ£ Clone the repository**
git clone (https://github.com/bindlish04/recomart-pipeline#)
cd recomart-pipeline

**2ï¸âƒ£ Create and activate virtual environment**
Windows (PowerShell):
py -m venv .venv
.\.venv\Scripts\activate

**3ï¸âƒ£ Install dependencies**
pip install -r requirements.txt

**ğŸ“¦ Data Handling (DVC)
Option A â€” Pull existing data (if you have access to DVC remote)**
dvc pull
**Option B â€” Regenerate data locally (no remote access needed)**
The pipeline will regenerate all data from ingestion onward.

**â–¶ï¸ Run the Full Pipeline (Recommended)**
This runs everything end-to-end:
python -m src.orchestration.prefect_flow

Pipeline stages:
CSV & API ingestion
Data validation + Data Quality PDF
Data preparation + EDA
Feature engineering + warehouse materialization
Model training & evaluation
MLflow logging

**ğŸ“Š View Experiment Tracking (MLflow)**
After running training/evaluation:
mlflow ui

Open the URL shown (usually http://127.0.0.1:5000) to view:
Parameters
Metrics (Precision@K, Recall@K, NDCG@K)
Model artifacts

**ğŸ§  Feature Store Usage (Example)**
python -m src.feature_store.demo_retrieve_features

Demonstrates:
Feature registry
Online-style feature retrieval for users/items

**ğŸ“ˆ Model Overview**
**Model type:** Popularity + Co-occurrence recommender
**Features:** User & item aggregates (7-day windows)
**Evaluation metrics:**
Precision@K
Recall@K
NDCG@K
**Tracking:** MLflow

This simple model is intentionally chosen to focus on data pipeline quality, not model complexity.

**ğŸ” Data Versioning & Lineage**

All pipeline datasets are tracked using DVC
Git commits reference dataset versions via .dvc files
Lineage is documented in docs/lineage.md
Dataset update workflow is documented in docs/dvc_workflow.md

**ğŸ“„ Key Documentation**
Raw storage design: docs/storage_structure.md
Feature engineering logic: docs/feature_logic.md
Feature store design: docs/feature_store.md
DVC workflow: docs/dvc_workflow.md
Lineage tracking: docs/lineage.md
Orchestration: docs/orchestration.md
